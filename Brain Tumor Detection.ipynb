{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Detection Using a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_brain_contour(image, plot=False):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "    thresh = cv2.erode(thresh, None, iterations=2)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "    \n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "    \n",
    "    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "    extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "    extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "    extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "    \n",
    "    new_image = image[extTop[1]:extBot[1], extLeft[0]:extRight[0]]\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_list, image_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    image_width, image_height = image_size\n",
    "    \n",
    "    for directory in dir_list:\n",
    "        for filename in listdir(directory):\n",
    "            image = cv2.imread(directory + '\\\\' + filename)\n",
    "            image = crop_brain_contour(image, plot=False)\n",
    "            image = cv2.resize(image, dsize=(image_width, image_height), interpolation=cv2.INTER_CUBIC)\n",
    "            image = image / 255.\n",
    "            X.append(image)\n",
    "            if directory[-3:] == 'yes':\n",
    "                y.append([1])\n",
    "            else:\n",
    "                y.append([0])\n",
    "                \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X, y = shuffle(X, y)\n",
    "    \n",
    "    print(f'Number of examples is: {len(X)}')\n",
    "    print(f'X shape is: {X.shape}')\n",
    "    print(f'y shape is: {y.shape}')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples is: 2065\n",
      "X shape is: (2065, 240, 240, 3)\n",
      "y shape is: (2065, 1)\n"
     ]
    }
   ],
   "source": [
    "augmented_path = 'augmented data/'\n",
    "augmented_yes = augmented_path + 'yes' \n",
    "augmented_no = augmented_path + 'no'\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = (240, 240)\n",
    "\n",
    "X, y = load_data([augmented_yes, augmented_no], (IMG_WIDTH, IMG_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size=0.2):\n",
    "    \n",
    "    X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=test_size)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1445\n",
      "number of development examples = 310\n",
      "number of test examples = 310\n",
      "X_train shape: (1445, 240, 240, 3)\n",
      "Y_train shape: (1445, 1)\n",
      "X_val (dev) shape: (310, 240, 240, 3)\n",
      "Y_val (dev) shape: (310, 1)\n",
      "X_test shape: (310, 240, 240, 3)\n",
      "Y_test shape: (310, 1)\n"
     ]
    }
   ],
   "source": [
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of development examples = \" + str(X_val.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_val (dev) shape: \" + str(X_val.shape))\n",
    "print (\"Y_val (dev) shape: \" + str(y_val.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m}:{round(s,1)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \n",
    "    X_input = Input(input_shape) \n",
    "    X = ZeroPadding2D((2, 2))(X_input) \n",
    "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X) \n",
    "    X = MaxPooling2D((4, 4), name='max_pool0')(X)  \n",
    "    X = MaxPooling2D((4, 4), name='max_pool1')(X) \n",
    "    X = Flatten()(X) \n",
    "    X = Dense(1, activation='sigmoid', name='fc')(X) \n",
    "    model = Model(inputs = X_input, outputs = X, name='BrainDetectionModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Varun Sirpal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = build_model(IMG_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BrainDetectionModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 240, 240, 3)]     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 244, 244, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 238, 238, 32)      4736      \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 238, 238, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 238, 238, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pool0 (MaxPooling2D)     (None, 59, 59, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pool1 (MaxPooling2D)     (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 1)                 6273      \n",
      "=================================================================\n",
      "Total params: 11,137\n",
      "Trainable params: 11,073\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Varun Sirpal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"cnn-parameters-improvement-{val_acc:.2f}\"\n",
    "checkpoint = ModelCheckpoint(\"models_2/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1445 samples, validate on 310 samples\n",
      "Epoch 1/10\n",
      "1445/1445 [==============================] - 215s 149ms/sample - loss: 0.7889 - acc: 0.6118 - val_loss: 0.7076 - val_acc: 0.4645\n",
      "Epoch 2/10\n",
      "1445/1445 [==============================] - 241s 167ms/sample - loss: 0.4961 - acc: 0.7640 - val_loss: 0.6889 - val_acc: 0.5097\n",
      "Epoch 3/10\n",
      "1445/1445 [==============================] - 242s 167ms/sample - loss: 0.5166 - acc: 0.7592 - val_loss: 0.6648 - val_acc: 0.5581\n",
      "Epoch 4/10\n",
      "1445/1445 [==============================] - 241s 167ms/sample - loss: 0.4317 - acc: 0.8125 - val_loss: 0.7475 - val_acc: 0.5097\n",
      "Epoch 5/10\n",
      "1445/1445 [==============================] - 234s 162ms/sample - loss: 0.3302 - acc: 0.8533 - val_loss: 0.9574 - val_acc: 0.4871\n",
      "Epoch 6/10\n",
      "1445/1445 [==============================] - 194s 134ms/sample - loss: 0.3015 - acc: 0.8754 - val_loss: 0.6891 - val_acc: 0.5935\n",
      "Epoch 7/10\n",
      "1445/1445 [==============================] - 193s 134ms/sample - loss: 0.2973 - acc: 0.8775 - val_loss: 0.6078 - val_acc: 0.6613\n",
      "Epoch 8/10\n",
      "1445/1445 [==============================] - 191s 132ms/sample - loss: 0.3041 - acc: 0.8692 - val_loss: 0.8762 - val_acc: 0.6097\n",
      "Epoch 9/10\n",
      "1445/1445 [==============================] - 212s 146ms/sample - loss: 0.2776 - acc: 0.8872 - val_loss: 0.4193 - val_acc: 0.7968\n",
      "Epoch 10/10\n",
      "1445/1445 [==============================] - 203s 140ms/sample - loss: 0.2091 - acc: 0.9225 - val_loss: 0.6012 - val_acc: 0.7065\n",
      "Elapsed time: 0:36:6.7\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val),callbacks=[checkpoint])\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)\n",
    "print(f\"Elapsed time: {hms_string(execution_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train for a few more epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1445 samples, validate on 310 samples\n",
      "Epoch 1/4\n",
      "1445/1445 [==============================] - 198s 137ms/sample - loss: 0.1967 - acc: 0.9308 - val_loss: 0.7143 - val_acc: 0.6581\n",
      "Epoch 2/4\n",
      "1445/1445 [==============================] - 199s 138ms/sample - loss: 0.2318 - acc: 0.8983 - val_loss: 0.4702 - val_acc: 0.7871\n",
      "Epoch 3/4\n",
      "1445/1445 [==============================] - 194s 134ms/sample - loss: 0.1870 - acc: 0.9273 - val_loss: 0.5096 - val_acc: 0.7774\n",
      "Epoch 4/4\n",
      "1445/1445 [==============================] - 217s 150ms/sample - loss: 0.1755 - acc: 0.9391 - val_loss: 0.5865 - val_acc: 0.7452\n",
      "Elapsed time: 0:13:27.8\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=4, validation_data=(X_val, y_val),callbacks=[checkpoint])\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)\n",
    "print(f\"Elapsed time: {hms_string(execution_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1445 samples, validate on 310 samples\n",
      "Epoch 1/5\n",
      "1445/1445 [==============================] - 211s 146ms/sample - loss: 0.1581 - acc: 0.9384 - val_loss: 0.8055 - val_acc: 0.6742\n",
      "Epoch 2/5\n",
      "1445/1445 [==============================] - 218s 151ms/sample - loss: 0.1715 - acc: 0.9419 - val_loss: 0.3907 - val_acc: 0.8226\n",
      "Epoch 3/5\n",
      "1445/1445 [==============================] - 106s 73ms/sample - loss: 0.1598 - acc: 0.9377 - val_loss: 0.4646 - val_acc: 0.8194\n",
      "Epoch 4/5\n",
      "1445/1445 [==============================] - 102s 71ms/sample - loss: 0.1231 - acc: 0.9571 - val_loss: 0.3698 - val_acc: 0.8387\n",
      "Epoch 5/5\n",
      "1445/1445 [==============================] - 104s 72ms/sample - loss: 0.1371 - acc: 0.9481 - val_loss: 0.3570 - val_acc: 0.8484\n",
      "Elapsed time: 0:12:20.5\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=5, validation_data=(X_val, y_val),callbacks=[checkpoint])\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)\n",
    "print(f\"Elapsed time: {hms_string(execution_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1445 samples, validate on 310 samples\n",
      "Epoch 1/5\n",
      "1445/1445 [==============================] - 110s 76ms/sample - loss: 0.1217 - acc: 0.9536 - val_loss: 0.7190 - val_acc: 0.7323\n",
      "Epoch 2/5\n",
      "1445/1445 [==============================] - 112s 77ms/sample - loss: 0.1158 - acc: 0.9592 - val_loss: 0.3542 - val_acc: 0.8613\n",
      "Epoch 3/5\n",
      "1445/1445 [==============================] - 108s 75ms/sample - loss: 0.1026 - acc: 0.9640 - val_loss: 0.4349 - val_acc: 0.7968\n",
      "Epoch 4/5\n",
      "1445/1445 [==============================] - 109s 76ms/sample - loss: 0.1240 - acc: 0.9536 - val_loss: 0.3905 - val_acc: 0.8323\n",
      "Epoch 5/5\n",
      "1445/1445 [==============================] - 110s 76ms/sample - loss: 0.0863 - acc: 0.9737 - val_loss: 0.4291 - val_acc: 0.8194\n",
      "Elapsed time: 0:9:9.0\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=5, validation_data=(X_val, y_val),callbacks=[checkpoint])\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)\n",
    "print(f\"Elapsed time: {hms_string(execution_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1445 samples, validate on 310 samples\n",
      "Epoch 1/6\n",
      "1445/1445 [==============================] - 109s 76ms/sample - loss: 0.0799 - acc: 0.9799 - val_loss: 0.3289 - val_acc: 0.8677\n",
      "Epoch 2/6\n",
      "1445/1445 [==============================] - 110s 76ms/sample - loss: 0.0656 - acc: 0.9896 - val_loss: 0.4067 - val_acc: 0.8677\n",
      "Epoch 3/6\n",
      "1445/1445 [==============================] - 111s 77ms/sample - loss: 0.0796 - acc: 0.9806 - val_loss: 0.3512 - val_acc: 0.8742\n",
      "Epoch 4/6\n",
      "1445/1445 [==============================] - 109s 76ms/sample - loss: 0.0555 - acc: 0.9896 - val_loss: 0.3434 - val_acc: 0.8806\n",
      "Epoch 5/6\n",
      "1445/1445 [==============================] - 110s 76ms/sample - loss: 0.0522 - acc: 0.9945 - val_loss: 0.6051 - val_acc: 0.7710\n",
      "Epoch 6/6\n",
      "1445/1445 [==============================] - 110s 76ms/sample - loss: 0.0466 - acc: 0.9958 - val_loss: 0.3398 - val_acc: 0.8903\n",
      "Elapsed time: 0:11:0.1\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=6, validation_data=(X_val, y_val),callbacks = [checkpoint])\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)\n",
    "print(f\"Elapsed time: {hms_string(execution_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1445 samples, validate on 310 samples\n",
      "Epoch 1/5\n",
      "1445/1445 [==============================] - 120s 83ms/sample - loss: 0.0454 - acc: 0.9965 - val_loss: 0.4930 - val_acc: 0.7968\n",
      "Epoch 2/5\n",
      "1445/1445 [==============================] - 106s 73ms/sample - loss: 0.0432 - acc: 0.9945 - val_loss: 0.3782 - val_acc: 0.8806\n",
      "Epoch 3/5\n",
      "1445/1445 [==============================] - 103s 71ms/sample - loss: 0.0461 - acc: 0.9945 - val_loss: 0.3463 - val_acc: 0.8710\n",
      "Epoch 4/5\n",
      "1445/1445 [==============================] - 103s 71ms/sample - loss: 0.0360 - acc: 0.9979 - val_loss: 0.3613 - val_acc: 0.8935\n",
      "Epoch 5/5\n",
      "1445/1445 [==============================] - 103s 71ms/sample - loss: 0.0329 - acc: 0.9979 - val_loss: 0.4530 - val_acc: 0.8258\n",
      "Elapsed time: 0:8:54.9\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=5, validation_data=(X_val, y_val),callbacks=[checkpoint])\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)\n",
    "print(f\"Elapsed time: {hms_string(execution_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1445 samples, validate on 310 samples\n",
      "Epoch 1/5\n",
      "1445/1445 [==============================] - 103s 71ms/sample - loss: 0.0324 - acc: 0.9979 - val_loss: 0.8397 - val_acc: 0.7258\n",
      "Epoch 2/5\n",
      "1445/1445 [==============================] - 104s 72ms/sample - loss: 0.0607 - acc: 0.9841 - val_loss: 0.8318 - val_acc: 0.7581\n",
      "Epoch 3/5\n",
      "1445/1445 [==============================] - 103s 71ms/sample - loss: 0.0435 - acc: 0.9938 - val_loss: 0.3725 - val_acc: 0.8742\n",
      "Epoch 4/5\n",
      "1445/1445 [==============================] - 104s 72ms/sample - loss: 0.0489 - acc: 0.9903 - val_loss: 0.3753 - val_acc: 0.8516\n",
      "Epoch 5/5\n",
      "1445/1445 [==============================] - 110s 76ms/sample - loss: 0.0319 - acc: 0.9979 - val_loss: 0.4487 - val_acc: 0.8355\n",
      "Elapsed time: 0:8:43.8\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=5, validation_data=(X_val, y_val),callbacks=[checkpoint])\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)\n",
    "print(f\"Elapsed time: {hms_string(execution_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "acc\n",
      "val_loss\n",
      "val_acc\n"
     ]
    }
   ],
   "source": [
    "for key in history.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Varun Sirpal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Varun Sirpal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Varun Sirpal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model(filepath='models_2/cnn-parameters-improvement-0.89.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best model on the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 8s 26ms/sample - loss: 0.2183 - acc: 0.9129\n"
     ]
    }
   ],
   "source": [
    "loss, acc = best_model.evaluate(x=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the best model on the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.2183035294375112\n",
      "Test Accuracy = 91.29032492637634%\n"
     ]
    }
   ],
   "source": [
    "print (f\"Test Loss = {loss}\")\n",
    "print (f\"Test Accuracy = {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain tumor detected.\n"
     ]
    }
   ],
   "source": [
    "test_img = cv2.imread(\"yes/Y1.jpg\")\n",
    "test_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2RGB)\n",
    "test_img = cv2.resize(test_img,(240,240))\n",
    "test_img = np.array(test_img).reshape((-1,240,240,3))\n",
    "\n",
    "prediction = model.predict(test_img)\n",
    "if int(prediction[0][0])==1:\n",
    "    print(\"Brain tumor detected.\")\n",
    "else:\n",
    "    print(\"No brain tumor present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
